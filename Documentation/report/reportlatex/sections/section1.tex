%----------------------------------------------------------------------------------------
%	Chapter 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\section{Aim}
The objective of this project is to recognise handwritten digits by solving it as a classification problem using neural networks. The weights of the neural network are trained using back propagation (BP) algorithm and topology is obtained using genetic algorithm (GA).  The input training data has 25 features and the output is a digit between 0 and 9. The 25 features represent an encoded version of the pixels from the handwritten digit image. The training data has 60000 entries. The test dataset contains all the fields available in the train dataset except the output digit.
\section{Method \& Results}
\subsection{Back Propagation}
Back propagation is used to train the weights of a feed forward fully layered and fully connected neural network with 1 hidden layer with 100 hidden nodes. The input layer has 25 nodes  and the output layer has 10 nodes , one for classifying each digit from 0-9.   The network is designed to use hyper tan activation function for all layers and cross entropy loss cost function. The pseudo code and the corresponding implementation in the source code is detailed below.
\lstset{language=java, caption=Pseudo Code: Back Propagation, , label=code:BackPropagation}
\begin{lstlisting}
/*All code items listed are in project folder AE.MachineLearning.NeuralNet.Core
The feed forward network is implemented in NeuralNetwork.cs.
The details of the network implementation are  not provided here for brevity
Back propagation implemented in BackPropagationTraining.cs
Input network is pre-initialised with small [0.0 - 1.0) random weights, NeuralNetwork.cs, line 185
The network uses Tanh activation function for all layers, in HyperTanActivation.cs*/
Train (network, inputs[][], outputs[][], learningRate, momentum, maxError,  maxIterations)
do{
	//Initialise
	error = 0.0;
	iteration = 1;
	//Shuffle the input to minimise order related issues
	//implemented in Shuffler.cs
	ShuffleTrainData (inputs, outputs);	
	//Stochastic Gradient, use one item at a time to update the weights	
	for(each item in shuffledTrainingData ){	
		// Compute neural network output for item, NeuralNetwork.cs, line 185
		ComputeOutput(item)	
		//Compute gradient for the output layer, in line 234 in BackPropagationTraining.cs
		//EntropyLossGradientCalc.cs implements the Cross Entropy loss				
		ComputerGradientForOutputLayer()
		//Compute gradients for the rest of the layers , line 256 in BackPropagationTraining.cs
		 for (layer  l in N-1, N-2.....2){
		      ComputerGradientForHiddenLayer(l)
		 }		 
		 //Update the Weight and biases, line 280 in BackPropagationTraining.cs
		UpdateWeightsAndBias(learningRate, momentum, gradients);
	}		
	//Predict or validate using the train data
	predictedOutputs = Predict (shuffledTrainingDataInput)     
	//Calculate percentage error, ClassificationErrorCalculator.cs
	error = CalculateError(shuffledTrainingDataOutput, predictedOutputs)        
	iteration++;
}while (error > maxError && iteration < maxIterations)
\end{lstlisting}
	
 \subsection{Genetic Algorithm}
The genetic algorithm is used to obtain the best topology of a feed forward fully layered and fully connected neural network to recognise the hand written digits. The range of hidden layers is bounded between 1-10 and the range of number of neurons in each hidden layer is limited to between 1-100 and was run for 10 generations in this experiment. The input and output layer nodes are the same as previously specified for back propagation. The pseudo code and the corresponding source file is detailed below
\lstset{language=java, caption=Pseudo Code: Genetic Algorithm to optimise topology, , label=code:geneticAlgorithm}
\begin{lstlisting}
//All code items listed are in project folder AE.MachineLearning.NeuralNet.GeneticAlgorithms
//Implemented in GeneticAlgorithm.cs
Optimise( inputs, outputs, minLayer, maxLayer, minNodes, maxNodes, populationSize, totalGenerations)

//Initialise
generation = 1;
//Create Initial population by randomly sampling the solution space, Sampler.cs
//The solution space is 10 (number Of Hidden layers)^100 (number of nodes per hidden layer)
networkPopulation = SampleNetworkTopologies(populationSize);	
do{
	for(each network in networkPopulation ){	
		// Train the network using Back propagation, line 121 GeneticAlgorithm.cs
		trainNetwork(network, inputs, outputs)
		//Calculate Fitness by calculating percentage correctly classified, ClassficationFitnessCalculator.cs
	 	score[network] = CalculateFitness(network)
	}	
	//Select top n  ( in this case top half) fittest networks , RankBasedSelector.cs
	topnFitessNetworks = SelectFittestNetworks(.5 * populationSize)
	//From the fittest networks, randomly select n parents  and  mutate to create offspring, 
	//where n = .8 * num of fittest in this implementation, line 104, GeneticAlgorithm.cs
	//Mutation changes the no of nodes for a randomly selected layer, in Mutator.cs
	offspringMutatedNetworks = MutateToCreateOffspring(.8 * countOfFittestNetworks)
	//Inject new networks by random sampling, line 107 GeneticAlgorithm.cs
	randomNetworks = SampleNetworkTopologies(.2 * countOfFittestNetworks)
	//Merge all 3 network to create	the new population
	networkPopulation = Merge(topnFitessNetworks, offspringMutatedNetworks, randomNetworks)
	generation++

}while (generation < totalGenerations)
//Return the fittest networks from the last generation
return networkPopulation
\end{lstlisting}
\subsection{Results}
The training data was split into 70\% train and 30\% test to compute the scores. \\
For back propagation, a learning rate of .0001 was used and the best momentum of .075 was obtained by trial and error. The best network was able to predict  81.5\%  correctly (data not used during training).\\
The best network topology obtained using genetic algorithm had 1 single hidden layer with 90 nodes. This network, after training with back propagation was able to predict  80.5\%  correctly (data not used during training).
\section{Discussion \& Conclusion}
We have implemented stochastic gradient descent version of BP,  using each training data item to update the weights. The main factors affecting the accuracy of BP are the initial weights, learning rate and momentum. If all the weights are initialised to the same value, the network will never learn as the gradient will be same. Hence the weights must be randomly initialised.  The learning rate, when too high, prevents the network from learning and a learning rate of very low value makes the learning process extremely slow. Momentum, used to minimise the problem of local minima has to be obtained through trial and error.
The order of the input data also affects the accuracy for stochastic gradient descent. For instance if  the training data is ordered by the output class it belongs it, then the network tends to optimise for one specific class and the would be almost be overridden  when it sees inputs belonging to the next class.  In this implementation, the inputs are shuffled before each iteration.  \\In this implementation of GA to obtain the best network topology, we have used a single parent mutation to produce the offspring. Crossover was not used to produce offsprings due to the issues associated with structural symmetries associated with networks \cite{Branke1995}. We have also not encoded the network as a chromosome, due to time constraints, however the type of encoding used also affects the performance of GA \cite{Koehn1994}. The number of generations, the mutation rate and the randomly sampled solution injection rate impact the optimimum result produced by GA. For instance if the random sampling rate or the mutation rate is too high, then the best network may not survive, and on the other hand if it is too low then the optimum solution may never be found. Genetic algorithms are extremely good at preserving the fittest solution, as shown in  fig \ref{fig:Gagen} and can arrive at the optimum solution as long as at least one instance of the optimum solution is selected during random sampling. The best network identified by GA has a single hidden layer, and this is inline with the rule of thumb of 1-2 hidden layers for most neural networks.

\begin{figure}[h]
\centering
   \includegraphics[scale=.7]{figures/generations.pdf}
  \caption[Genetic Algorithm Generations Of Topologies]{Genetic Algorithm Generations Of Topologies: The first generation consists of randomly created network topologies. A very fit network topology with one hidden layer created randomly in the second generation multiplies survives through all generations and is dominant in the final generation 8} 
 \label{fig:Gagen}
\end{figure}

\section{Further Improvements}
Learning rate used in BP can be dynamic using adaptive learning rate techniques. NeuroEvolution of Augmenting Topologies (NEAT) algorithm  \cite{K2002} to create the optimum network by starting with the smallest network topology and gradually increasing the number of nodes and layers.

